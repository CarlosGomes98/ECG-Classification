{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras import optimizers, losses, activations, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, GlobalAveragePooling1D, \\\n",
    "    concatenate, Add, Activation\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, roc_auc_score, precision_recall_curve, auc, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/mitbih_train.csv\", header=None)\n",
    "df_train = df_train.sample(frac=1)\n",
    "df_test = pd.read_csv(\"data/mitbih_test.csv\", header=None)\n",
    "\n",
    "Y = np.array(df_train[187].values).astype(np.int8)\n",
    "X = np.array(df_train[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "Y_test = np.array(df_test[187].values).astype(np.int8)\n",
    "X_test = np.array(df_test[list(range(187))].values)[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = ['baseline/baseline_cnn_mitbih', 'paper/cnn_paper_mitbih', 'rnn/rnn_mitbih', 'deep_res/cnn_res_deep_mitbih']\n",
    "base_models = [load_model(model_name + '.h5') for model_name in base_models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One apprach: Take the softmax outputs, sum them up for each model, take the largest class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_ensemble_predict(models, X):\n",
    "    predicted = np.array([model.predict(X) for model in models])\n",
    "    predicted = predicted.sum(axis=0)\n",
    "    return np.argmax(predicted, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Will take a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 score : 0.9233999817799375 \n",
      "Test accuracy score : 0.9879864790791156 \n",
      "[[18086    19     9     2     2]\n",
      " [  107   442     6     0     1]\n",
      " [   37     5  1387    16     3]\n",
      " [   27     0    16   119     0]\n",
      " [   13     0     0     0  1595]]\n"
     ]
    }
   ],
   "source": [
    "predictions = simple_ensemble_predict(base_models, X_test)\n",
    "f1 = f1_score(Y_test, predictions, average=\"macro\")\n",
    "\n",
    "print(\"Test f1 score : %s \"% f1)\n",
    "\n",
    "acc = accuracy_score(Y_test, predictions)\n",
    "\n",
    "print(\"Test accuracy score : %s \"% acc)\n",
    "\n",
    "print(confusion_matrix(\n",
    "    Y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second approach: Take the softmax outputs as inputs to a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "newX = np.array([model.predict(X) for model in base_models])\n",
    "# reshape such that rows are samples and columns are the outputs from all of the networks concatenated\n",
    "newX = np.transpose(newX, (1, 0, 2)).reshape(X.shape[0], -1)\n",
    "\n",
    "newX_test = np.array([model.predict(X_test) for model in base_models])\n",
    "# reshape such that rows are samples and columns are the outputs from all of the networks concatenated\n",
    "newX_test = np.transpose(newX_test, (1, 0, 2)).reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression().fit(newX, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 score : 0.9227097706614031 \n",
      "Test accuracy score : 0.9876210487849443 \n",
      "[[18055    33    21     4     5]\n",
      " [   89   458     7     1     1]\n",
      " [   30     6  1396    14     2]\n",
      " [   30     0    14   118     0]\n",
      " [   14     0     0     0  1594]]\n"
     ]
    }
   ],
   "source": [
    "predictions = log.predict(newX_test)\n",
    "f1 = f1_score(Y_test, predictions, average=\"macro\")\n",
    "\n",
    "print(\"Test f1 score : %s \"% f1)\n",
    "\n",
    "acc = accuracy_score(Y_test, predictions)\n",
    "\n",
    "print(\"Test accuracy score : %s \"% acc)\n",
    "\n",
    "print(confusion_matrix(\n",
    "    Y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   40.6s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  60 | elapsed:  4.8min remaining:   44.1s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  5.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=42, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'C': [0.01, 0.1, 1],\n",
       "                         'class_weight': [None, 'balanced'],\n",
       "                         'gamma': ['auto', 'scale'], 'kernel': ['rbf']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=make_scorer(f1_score, average=macro), verbose=10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'C' : [0.01, 0.1, 1],\n",
    "    'kernel' : ['rbf'],\n",
    "    'gamma' : ['auto', 'scale'],\n",
    "    'class_weight' : [None, 'balanced']\n",
    "}\n",
    "clf = GridSearchCV(SVC(random_state=42),\n",
    "                   params,\n",
    "                   cv=5,\n",
    "                   refit=True,\n",
    "                   n_jobs=-1,\n",
    "                   scoring=make_scorer(f1_score, average='macro'),\n",
    "                  verbose=10)\n",
    "clf.fit(newX, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 score : 0.9237115143111663 \n",
      "Test accuracy score : 0.9873926548510872 \n",
      "[[18050    37    20     4     7]\n",
      " [   87   459     8     1     1]\n",
      " [   32     6  1389    18     3]\n",
      " [   28     0    11   123     0]\n",
      " [   13     0     0     0  1595]]\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(newX_test)\n",
    "f1 = f1_score(Y_test, predictions, average=\"macro\")\n",
    "\n",
    "print(\"Test f1 score : %s \"% f1)\n",
    "\n",
    "acc = accuracy_score(Y_test, predictions)\n",
    "\n",
    "print(\"Test accuracy score : %s \"% acc)\n",
    "\n",
    "print(confusion_matrix(\n",
    "    Y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We take the same approach for the PTBDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"data/ptbdb_normal.csv\", header=None)\n",
    "df_2 = pd.read_csv(\"data/ptbdb_abnormal.csv\", header=None)\n",
    "df = pd.concat([df_1, df_2])\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=1337, stratify=df[187])\n",
    "\n",
    "Y = np.array(df_train[187].values).astype(np.int8)\n",
    "X = np.array(df_train[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "Y_test = np.array(df_test[187].values).astype(np.int8)\n",
    "X_test = np.array(df_test[list(range(187))].values)[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = ['baseline/baseline_cnn_ptbdb', 'paper/cnn_paper_ptbdb', 'deep_res/cnn_res_deep_ptbdb']# 'rnn/rnn_ptbdb'\n",
    "base_models = [load_model(model_name + '.h5') for model_name in base_models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One apprach: Take the softmax outputs, sum them up for each model, take the largest class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_ensemble_predict(models, X):\n",
    "    predicted = np.array([model.predict(X) for model in models])\n",
    "    predicted = predicted.sum(axis=0)/len(base_models)\n",
    "    return (predicted>0.5).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 score : 0.9948527374854859 \n",
      "Test accuracy score : 0.9958777052559258 \n",
      "AUROC score : 0.9937239711664329 \n",
      "AUPRC score : 0.9976669558958856 \n",
      "[[ 800    9]\n",
      " [   3 2099]]\n"
     ]
    }
   ],
   "source": [
    "predictions = simple_ensemble_predict(base_models, X_test)\n",
    "f1 = f1_score(Y_test, predictions, average=\"macro\")\n",
    "\n",
    "print(\"Test f1 score : %s \"% f1)\n",
    "\n",
    "acc = accuracy_score(Y_test, predictions)\n",
    "\n",
    "print(\"Test accuracy score : %s \"% acc)\n",
    "\n",
    "auc_roc = roc_auc_score(Y_test, predictions)\n",
    "\n",
    "print(\"AUROC score : %s \"% auc_roc)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(Y_test, predictions)\n",
    "\n",
    "auc_prc = auc(recall, precision)\n",
    "print(\"AUPRC score : %s \"% auc_prc)\n",
    "\n",
    "print(confusion_matrix(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second approach: Take the softmax outputs as inputs to a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "newX_pt = np.array([model.predict(X) for model in base_models])\n",
    "# reshape such that rows are samples and columns are the outputs from all of the networks concatenated\n",
    "newX_pt = np.transpose(newX_pt, (1, 0, 2)).reshape(X.shape[0], -1)\n",
    "\n",
    "newX_test_pt = np.array([model.predict(X_test) for model in base_models])\n",
    "# reshape such that rows are samples and columns are the outputs from all of the networks concatenated\n",
    "newX_test_pt = np.transpose(newX_test_pt, (1, 0, 2)).reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression().fit(newX_pt, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 score : 0.9948566723677329 \n",
      "Test accuracy score : 0.9958777052559258 \n",
      "AUROC score : 0.994104149441523 \n",
      "AUPRC score : 0.9978362391054255 \n",
      "[[ 801    8]\n",
      " [   4 2098]]\n"
     ]
    }
   ],
   "source": [
    "predictions = log.predict(newX_test_pt)\n",
    "f1 = f1_score(Y_test, predictions, average=\"macro\")\n",
    "\n",
    "print(\"Test f1 score : %s \"% f1)\n",
    "\n",
    "acc = accuracy_score(Y_test, predictions)\n",
    "\n",
    "print(\"Test accuracy score : %s \"% acc)\n",
    "\n",
    "auc_roc = roc_auc_score(Y_test, predictions)\n",
    "\n",
    "print(\"AUROC score : %s \"% auc_roc)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(Y_test, predictions)\n",
    "\n",
    "auc_prc = auc(recall, precision)\n",
    "print(\"AUPRC score : %s \"% auc_prc)\n",
    "\n",
    "print(confusion_matrix(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning for svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:   16.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=42, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=1,\n",
       "             param_grid={'C': [0.01, 0.1, 1],\n",
       "                         'class_weight': [None, 'balanced'],\n",
       "                         'gamma': ['auto', 'scale'],\n",
       "                         'kernel': ['rbf', 'poly']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=make_scorer(f1_score, average=macro), verbose=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'C' : [0.01, 0.1, 1],\n",
    "    'kernel' : ['rbf', 'poly'],\n",
    "    'gamma' : ['auto', 'scale'],\n",
    "    'class_weight' : [None, 'balanced']\n",
    "}\n",
    "clf = GridSearchCV(SVC(random_state=42),\n",
    "                   params,\n",
    "                   cv=5,\n",
    "                   refit=True,\n",
    "                   n_jobs=1,\n",
    "                   scoring=make_scorer(f1_score, average='macro'),\n",
    "                  verbose=1,\n",
    "                  return_train_score=True)\n",
    "clf.fit(newX_pt, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 score : 0.9927384934681365 \n",
      "Test accuracy score : 0.9941600824458949 \n",
      "AUROC score : 0.9936751625093061 \n",
      "AUPRC score : 0.9978422142652383 \n",
      "[[ 803    6]\n",
      " [  11 2091]]\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(newX_test_pt)\n",
    "f1 = f1_score(Y_test, predictions, average=\"macro\")\n",
    "\n",
    "print(\"Test f1 score : %s \"% f1)\n",
    "\n",
    "acc = accuracy_score(Y_test, predictions)\n",
    "\n",
    "print(\"Test accuracy score : %s \"% acc)\n",
    "\n",
    "auc_roc = roc_auc_score(Y_test, predictions)\n",
    "\n",
    "print(\"AUROC score : %s \"% auc_roc)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(Y_test, predictions)\n",
    "\n",
    "auc_prc = auc(recall, precision)\n",
    "print(\"AUPRC score : %s \"% auc_prc)\n",
    "\n",
    "print(confusion_matrix(Y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ECG] *",
   "language": "python",
   "name": "conda-env-ECG-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
