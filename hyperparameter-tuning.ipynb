{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.decomposition as decomposition\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import f1_score, accuracy_score, make_scorer\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input, optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import LSTM, GRU, BatchNormalization\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "#%load_ext tensorboard\n",
    "#from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((87554, 187, 1), (87554,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"data/mitbih_train.csv\", header=None)\n",
    "df_train = df_train.sample(frac=1)\n",
    "df_test = pd.read_csv(\"data/mitbih_test.csv\", header=None)\n",
    "\n",
    "Y = np.array(df_train[187].values).astype(np.int8)\n",
    "X = np.array(df_train[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "Y_test = np.array(df_test[187].values).astype(np.int8)\n",
    "X_test = np.array(df_test[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "n_class = np.unique(Y).size\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter search on RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru(n_class=5, dropout=0.3, rnn_sizes = [128, 128], fc_sizes=[64], batch_norm=True):\n",
    "    nclass = 5\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(187, 1)))\n",
    "    \n",
    "    if batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    for index, dim in enumerate(rnn_sizes):\n",
    "        model.add(GRU(dim, dropout=dropout, return_sequences=(index != len(rnn_sizes) - 1)))\n",
    "        \n",
    "        if batch_norm:\n",
    "            model.add(BatchNormalization())\n",
    "    \n",
    "    for index, dim in enumerate(fc_sizes):\n",
    "        model.add(Dense(dim, activation=\"relu\"))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        if batch_norm:\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "    model.add(Dense(nclass, activation=\"softmax\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNN(BaseEstimator):\n",
    "    def fit(self, train_X, train_y, **kwargs):\n",
    "        \n",
    "        self.build_model()\n",
    "        \n",
    "        # early = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=5, verbose=1)\n",
    "        # redonplat = ReduceLROnPlateau(monitor=\"val_accuracy\", mode=\"max\", patience=3, verbose=2)\n",
    "        # callbacks_list = [checkpoint, early, redonplat]  # early\n",
    "        self.model.fit(train_X, train_y, validation_split=0.1, epochs=self.epochs, batch_size=self.batch_size)\n",
    "    \n",
    "    def predict(self, eval_X):\n",
    "        return np.argmax(self.model.predict(eval_X), axis=1)\n",
    "    \n",
    "    def set_params(self, epochs=100, \n",
    "                         batch_size=64, \n",
    "                         learning_rate=1e-3, \n",
    "                         dropout=0.3, \n",
    "                         rnn_sizes=[128, 128],\n",
    "                         fc_sizes=[64],\n",
    "                         batch_norm=True):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout = dropout\n",
    "        self.rnn_sizes = rnn_sizes\n",
    "        self.fc_sizes = fc_sizes\n",
    "        self.batch_norm = batch_norm\n",
    "                \n",
    "        return self\n",
    "        \n",
    "    def build_model(self):\n",
    "        self.model = build_gru(n_class=5, dropout=self.dropout, \n",
    "                          rnn_sizes=self.rnn_sizes, fc_sizes=self.fc_sizes, batch_norm=self.batch_norm)\n",
    "        opt = optimizers.Adam(self.learning_rate)\n",
    "            \n",
    "        self.model.compile(optimizer=opt, \n",
    "                      loss=\"sparse_categorical_crossentropy\", \n",
    "                      metrics=['accuracy'])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] batch_norm=True, batch_size=64, dropout=0.2, epochs=20, fc_sizes=[64, 32], learning_rate=0.0001, rnn_sizes=[128, 128] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63038 samples, validate on 7005 samples\n",
      "Epoch 1/20\n",
      "63038/63038 [==============================] - 79s 1ms/sample - loss: 1.6467 - accuracy: 0.3746 - val_loss: 1.3095 - val_accuracy: 0.8206\n",
      "Epoch 2/20\n",
      "63038/63038 [==============================] - 65s 1ms/sample - loss: 1.0468 - accuracy: 0.7165 - val_loss: 0.7695 - val_accuracy: 0.8213\n",
      "Epoch 3/20\n",
      "63038/63038 [==============================] - 64s 1ms/sample - loss: 0.8013 - accuracy: 0.8053 - val_loss: 0.6896 - val_accuracy: 0.8216\n",
      "Epoch 4/20\n",
      "63038/63038 [==============================] - 64s 1ms/sample - loss: 0.7361 - accuracy: 0.8176 - val_loss: 0.6492 - val_accuracy: 0.8217\n",
      "Epoch 5/20\n",
      "63038/63038 [==============================] - 64s 1ms/sample - loss: 0.6732 - accuracy: 0.8200 - val_loss: 0.5835 - val_accuracy: 0.8223\n",
      "Epoch 6/20\n",
      "63038/63038 [==============================] - 64s 1ms/sample - loss: 0.5891 - accuracy: 0.8272 - val_loss: 0.4873 - val_accuracy: 0.8444\n",
      "Epoch 7/20\n",
      "63038/63038 [==============================] - 64s 1ms/sample - loss: 0.4906 - accuracy: 0.8587 - val_loss: 0.3618 - val_accuracy: 0.9101\n",
      "Epoch 8/20\n",
      "63038/63038 [==============================] - 64s 1ms/sample - loss: 0.4127 - accuracy: 0.8887 - val_loss: 0.3914 - val_accuracy: 0.8884\n",
      "Epoch 9/20\n",
      "63038/63038 [==============================] - 64s 1ms/sample - loss: 0.3727 - accuracy: 0.9009 - val_loss: 0.3389 - val_accuracy: 0.9052\n",
      "Epoch 10/20\n",
      "63038/63038 [==============================] - 63s 1ms/sample - loss: 0.3520 - accuracy: 0.9059 - val_loss: 0.3478 - val_accuracy: 0.9065\n",
      "Epoch 11/20\n",
      "63038/63038 [==============================] - 64s 1ms/sample - loss: 0.3363 - accuracy: 0.9099 - val_loss: 0.3433 - val_accuracy: 0.8994\n",
      "Epoch 12/20\n",
      "63038/63038 [==============================] - 63s 1ms/sample - loss: 0.3237 - accuracy: 0.9131 - val_loss: 0.3611 - val_accuracy: 0.8965\n",
      "Epoch 13/20\n",
      "63038/63038 [==============================] - 72s 1ms/sample - loss: 0.3107 - accuracy: 0.9146 - val_loss: 0.2806 - val_accuracy: 0.9219\n",
      "Epoch 14/20\n",
      "63038/63038 [==============================] - 77s 1ms/sample - loss: 0.2995 - accuracy: 0.9178 - val_loss: 0.2827 - val_accuracy: 0.9162\n",
      "Epoch 15/20\n",
      "63038/63038 [==============================] - 70s 1ms/sample - loss: 0.2849 - accuracy: 0.9197 - val_loss: 0.2641 - val_accuracy: 0.9205\n",
      "Epoch 16/20\n",
      "63038/63038 [==============================] - 89s 1ms/sample - loss: 0.2759 - accuracy: 0.9214 - val_loss: 0.2698 - val_accuracy: 0.9191\n",
      "Epoch 17/20\n",
      "63038/63038 [==============================] - 70s 1ms/sample - loss: 0.2630 - accuracy: 0.9244 - val_loss: 0.2840 - val_accuracy: 0.9155\n",
      "Epoch 18/20\n",
      "63038/63038 [==============================] - 70s 1ms/sample - loss: 0.2525 - accuracy: 0.9286 - val_loss: 0.2396 - val_accuracy: 0.9283\n",
      "Epoch 19/20\n",
      "63038/63038 [==============================] - 67s 1ms/sample - loss: 0.2330 - accuracy: 0.9337 - val_loss: 0.2360 - val_accuracy: 0.9281\n",
      "Epoch 20/20\n",
      "63038/63038 [==============================] - 67s 1ms/sample - loss: 0.2200 - accuracy: 0.9383 - val_loss: 0.1849 - val_accuracy: 0.9433\n",
      "[CV]  batch_norm=True, batch_size=64, dropout=0.2, epochs=20, fc_sizes=[64, 32], learning_rate=0.0001, rnn_sizes=[128, 128], score=(train=0.620, test=0.619), total=23.0min\n",
      "[CV] batch_norm=True, batch_size=64, dropout=0.2, epochs=20, fc_sizes=[64, 32], learning_rate=0.0001, rnn_sizes=[128, 128] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 23.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63038 samples, validate on 7005 samples\n",
      "Epoch 1/20\n",
      "63038/63038 [==============================] - 82s 1ms/sample - loss: 1.6175 - accuracy: 0.3966 - val_loss: 1.1188 - val_accuracy: 0.8213\n",
      "Epoch 2/20\n",
      "14080/63038 [=====>........................] - ETA: 1:49 - loss: 1.2047 - accuracy: 0.6320"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'epochs': [2],\n",
    "    'batch_size': [64],\n",
    "    'learning_rate': [1e-3],\n",
    "    'dropout': [0.2],\n",
    "    'rnn_sizes': [[128, 128], [128, 128, 128]],\n",
    "    'fc_sizes': [[64], [64, 64], [64, 32]],\n",
    "    'batch_norm': [True, False]\n",
    "}\n",
    "\n",
    "dummy_params = {\n",
    "    'epochs': [20],\n",
    "    'batch_size': [64],\n",
    "    'learning_rate': [1e-4],\n",
    "    'dropout': [0.2],\n",
    "    'rnn_sizes': [[128, 128]],\n",
    "    'fc_sizes': [[64, 32]],\n",
    "    'batch_norm': [True]\n",
    "}\n",
    "\n",
    "model = CustomRNN()\n",
    "search = GridSearchCV(estimator=model, \n",
    "                      param_grid=dummy_params,\n",
    "                      scoring=make_scorer(f1_score, average='macro'),\n",
    "                      n_jobs=1, \n",
    "                      return_train_score=True, \n",
    "                      refit=False, \n",
    "                      verbose=10,\n",
    "                      error_score='raise')\n",
    "best = search.fit(X[:, :, :], Y[:])\n",
    "best.__dict__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
