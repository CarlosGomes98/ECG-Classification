{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.decomposition as decomposition\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import f1_score, accuracy_score, make_scorer\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "%load_ext tensorboard\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((87554, 187, 1), (87554,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"data/mitbih_train.csv\", header=None)\n",
    "df_train = df_train.sample(frac=1)\n",
    "df_test = pd.read_csv(\"data/mitbih_test.csv\", header=None)\n",
    "\n",
    "Y = np.array(df_train[187].values).astype(np.int8)\n",
    "X = np.array(df_train[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "Y_test = np.array(df_test[187].values).astype(np.int8)\n",
    "X_test = np.array(df_test[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "n_class = np.unique(Y).size\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter search on RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru(n_class=5, dropout=0.3, rnn_sizes = [128, 128], fc_sizes=[64], batch_norm=True):\n",
    "    nclass = 5\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(187, 1)))\n",
    "    \n",
    "    if batch_norm:\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    for index, dim in enumerate(rnn_sizes):\n",
    "        model.add(GRU(dim, dropout=dropout, return_sequences=(index != len(rnn_sizes) - 1)))\n",
    "        \n",
    "        if batch_norm:\n",
    "            model.add(BatchNormalization())\n",
    "    \n",
    "    for index, dim in enumerate(fc_sizes):\n",
    "        model.add(Dense(dim, dropout=dropout, activation=\"relu\"))\n",
    "        \n",
    "        if batch_norm:\n",
    "            model.add(BatchNormalization())\n",
    "            \n",
    "    model.add(Dense(nclass, activation=\"softmax\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNN(BaseEstimator):\n",
    "    #def __init__():\n",
    "    #    super().__init__()\n",
    "    \n",
    "    def fit(self, train_X, train_y, **kwargs):\n",
    "        \n",
    "        self.build_model()\n",
    "        \n",
    "        early = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=5, verbose=1)\n",
    "        redonplat = ReduceLROnPlateau(monitor=\"val_accuracy\", mode=\"max\", patience=3, verbose=2)\n",
    "        callbacks_list = [checkpoint, early, redonplat]  # early\n",
    "        self.model.fit(train_X, train_y, validation_split=0.1, epochs=self.epochs, batch_size=self.batch_size)\n",
    "    \n",
    "    def predict(self, eval_X):\n",
    "        \n",
    "        return self.model.predict(eval_X)\n",
    "    \n",
    "    def set_params(self, epochs=100, \n",
    "                         batch_size=64, \n",
    "                         learning_rate=1e-3, \n",
    "                         dropout=0.3, \n",
    "                         rnn_sizes=[128, 128],\n",
    "                         fc_sizes=[64],\n",
    "                         batch_norm=True):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout = dropout\n",
    "        self.rnn_sizes = rnn_sizes\n",
    "        self.fc_sizes = fc_sizes\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def build_model(self):\n",
    "        self.model = build_gru(n_class=5, dropout=self.dropout, \n",
    "                          rnn_sizes=self.rnn_sizes, fc_sizes=self.fc_sizes, batch_norm=self.batch_norm)\n",
    "        opt = optimizers.Adam(self.learning_rate)\n",
    "            \n",
    "        self.model.compile(optimizer=opt, \n",
    "                      loss=\"sparse_categorical_crossentropy\", \n",
    "                      metrics=['accuracy'])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] batch_norm=True, batch_size=16, dropout=0.2, epochs=1, fc_sizes=[64], learning_rate=0.001, rnn_sizes=[64] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_norm=True, batch_size=16, dropout=0.2, epochs=1, fc_sizes=[64], learning_rate=0.001, rnn_sizes=[64], score=(train=nan, test=nan), total=   0.2s\n",
      "[CV] batch_norm=True, batch_size=16, dropout=0.2, epochs=1, fc_sizes=[64], learning_rate=0.001, rnn_sizes=[64] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_norm=True, batch_size=16, dropout=0.2, epochs=1, fc_sizes=[64], learning_rate=0.001, rnn_sizes=[64], score=(train=nan, test=nan), total=   0.2s\n",
      "[CV] batch_norm=True, batch_size=16, dropout=0.2, epochs=1, fc_sizes=[64], learning_rate=0.001, rnn_sizes=[64] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_norm=True, batch_size=16, dropout=0.2, epochs=1, fc_sizes=[64], learning_rate=0.001, rnn_sizes=[64], score=(train=nan, test=nan), total=   0.2s\n",
      "[CV] batch_norm=True, batch_size=16, dropout=0.2, epochs=1, fc_sizes=[64], learning_rate=0.001, rnn_sizes=[64] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_norm=True, batch_size=16, dropout=0.2, epochs=1, fc_sizes=[64], learning_rate=0.001, rnn_sizes=[64], score=(train=nan, test=nan), total=   0.2s\n",
      "[CV] batch_norm=True, batch_size=16, dropout=0.2, epochs=1, fc_sizes=[64], learning_rate=0.001, rnn_sizes=[64] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    1.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  batch_norm=True, batch_size=16, dropout=0.2, epochs=1, fc_sizes=[64], learning_rate=0.001, rnn_sizes=[64], score=(train=nan, test=nan), total=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'scoring': make_scorer(f1_score),\n",
       " 'estimator': CustomRNN(),\n",
       " 'n_jobs': 1,\n",
       " 'iid': 'deprecated',\n",
       " 'refit': False,\n",
       " 'cv': None,\n",
       " 'verbose': 10,\n",
       " 'pre_dispatch': '2*n_jobs',\n",
       " 'error_score': nan,\n",
       " 'return_train_score': True,\n",
       " 'param_grid': {'epochs': [1],\n",
       "  'batch_size': [16],\n",
       "  'learning_rate': [0.001],\n",
       "  'dropout': [0.2],\n",
       "  'rnn_sizes': [[64]],\n",
       "  'fc_sizes': [[64]],\n",
       "  'batch_norm': [True]},\n",
       " 'multimetric_': False,\n",
       " 'best_index_': 0,\n",
       " 'best_score_': nan,\n",
       " 'best_params_': {'batch_norm': True,\n",
       "  'batch_size': 16,\n",
       "  'dropout': 0.2,\n",
       "  'epochs': 1,\n",
       "  'fc_sizes': [64],\n",
       "  'learning_rate': 0.001,\n",
       "  'rnn_sizes': [64]},\n",
       " 'scorer_': make_scorer(f1_score),\n",
       " 'cv_results_': {'mean_fit_time': array([0.22329526]),\n",
       "  'std_fit_time': array([0.00708975]),\n",
       "  'mean_score_time': array([0.]),\n",
       "  'std_score_time': array([0.]),\n",
       "  'param_batch_norm': masked_array(data=[True],\n",
       "               mask=[False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_batch_size': masked_array(data=[16],\n",
       "               mask=[False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_dropout': masked_array(data=[0.2],\n",
       "               mask=[False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_epochs': masked_array(data=[1],\n",
       "               mask=[False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_fc_sizes': masked_array(data=[list([64])],\n",
       "               mask=[False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_learning_rate': masked_array(data=[0.001],\n",
       "               mask=[False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_rnn_sizes': masked_array(data=[list([64])],\n",
       "               mask=[False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'params': [{'batch_norm': True,\n",
       "    'batch_size': 16,\n",
       "    'dropout': 0.2,\n",
       "    'epochs': 1,\n",
       "    'fc_sizes': [64],\n",
       "    'learning_rate': 0.001,\n",
       "    'rnn_sizes': [64]}],\n",
       "  'split0_test_score': array([nan]),\n",
       "  'split1_test_score': array([nan]),\n",
       "  'split2_test_score': array([nan]),\n",
       "  'split3_test_score': array([nan]),\n",
       "  'split4_test_score': array([nan]),\n",
       "  'mean_test_score': array([nan]),\n",
       "  'std_test_score': array([nan]),\n",
       "  'rank_test_score': array([1], dtype=int32),\n",
       "  'split0_train_score': array([nan]),\n",
       "  'split1_train_score': array([nan]),\n",
       "  'split2_train_score': array([nan]),\n",
       "  'split3_train_score': array([nan]),\n",
       "  'split4_train_score': array([nan]),\n",
       "  'mean_train_score': array([nan]),\n",
       "  'std_train_score': array([nan])},\n",
       " 'n_splits_': 5}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'epochs': [100],\n",
    "    'batch_size': [64],\n",
    "    'learning_rate': [1e-3],\n",
    "    'dropout': [0.2],\n",
    "    'rnn_sizes': [[128, 128], [128, 128, 128], [128, 128, 128, 128], [256, 256], [256, 256, 256], [256, 256, 128]],\n",
    "    'fc_sizes': [[64], [64, 64], [64, 32]],\n",
    "    'batch_norm': [True, False]\n",
    "}\n",
    "\n",
    "dummy_params = {\n",
    "    'epochs': [1],\n",
    "    'batch_size': [16],\n",
    "    'learning_rate': [1e-3],\n",
    "    'dropout': [0.2],\n",
    "    'rnn_sizes': [[64]],\n",
    "    'fc_sizes': [[64]],\n",
    "    'batch_norm': [True]\n",
    "}\n",
    "\n",
    "model = CustomRNN()\n",
    "search = GridSearchCV(estimator=model, \n",
    "                      param_grid=dummy_params,\n",
    "                      scoring=make_scorer(f1_score),\n",
    "                      n_jobs=1, \n",
    "                      return_train_score=True, \n",
    "                      refit=True, \n",
    "                      verbose=10)\n",
    "best = search.fit(X, Y)\n",
    "best.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 187\n",
    "\n",
    "def get_model():\n",
    "    n_class = 5\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, \n",
    "                   input_shape=(seq_len, 1), \n",
    "                   activation='relu', \n",
    "                   return_sequences=True,\n",
    "                   dropout=0.2))\n",
    "    model.add(LSTM(128, \n",
    "                   activation='relu',\n",
    "                   return_sequences=False, \n",
    "                   dropout=0.1))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(n_class, activation='softmax'))\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "    \n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_simple_model():\n",
    "    n_class = 5\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, \n",
    "                   input_shape=(seq_len, 1), \n",
    "                   activation='relu', \n",
    "                   return_sequences=False,\n",
    "                   dropout=0.2))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(n_class, activation='softmax'))\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "    \n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 32)                4352      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 5,573\n",
      "Trainable params: 5,573\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_simple_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4], dtype=int8),\n",
       " array([848,  17,  58,  10,  67], dtype=int64))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_X = X[:1000, :, :]\n",
    "mini_Y = Y[:1000]\n",
    "\n",
    "np.unique(mini_Y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 900 samples, validate on 100 samples\n",
      "Epoch 1/2\n",
      "900/900 [==============================] - 13s 15ms/sample - loss: 1.5165 - accuracy: 0.8489 - val_loss: 1.3513 - val_accuracy: 0.8300\n",
      "Epoch 2/2\n",
      "900/900 [==============================] - 10s 11ms/sample - loss: nan - accuracy: 0.8500 - val_loss: nan - val_accuracy: 0.8300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25a100c7fc8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X[:1000, :, :], Y[:1000], \n",
    "          epochs=2, \n",
    "          verbose=1,\n",
    "          batch_size=32,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.305817  , 0.15159464, 0.1644598 , 0.19375268, 0.18437594],\n",
       "       [0.30581692, 0.15159465, 0.16445978, 0.19375265, 0.18437594],\n",
       "       [0.30581677, 0.15159471, 0.16445987, 0.19375265, 0.18437597],\n",
       "       ...,\n",
       "       [0.30581686, 0.15159468, 0.16445982, 0.19375265, 0.18437594],\n",
       "       [0.3058147 , 0.1515957 , 0.16446076, 0.19375263, 0.18437621],\n",
       "       [0.30581692, 0.15159465, 0.16445978, 0.19375265, 0.18437594]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(mini_X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
