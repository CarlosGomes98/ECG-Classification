{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual CNN from https://arxiv.org/abs/1805.00794\n",
    "This model is an implementation of the one presented in the paper.\n",
    "\n",
    "The model relies on repeated 1D convolutions to build lower dimensional representations of the output. \n",
    "The structure's main feature is the repetition of residual blocks. These are made up of two Convolutional layers, a residual connection from the input to the block, ReLU and MaxPooling.\n",
    "\n",
    "The dimensionality reduction is carried out using MaxPool layers, and each Conv layer maintains the same number of filters (32), which is somewhat unusual.\n",
    "\n",
    "The residual networks aid in backpropagation of the gradient, however, this model is perhaps not quite deep enough to warant these, as shown by the performance of the [Baseline CNN](../baseline/baseline_cnn_ptbdb.ipynb), which is essentially this model without the residual connections.\n",
    "\n",
    "From the performance of the [Deep Residual CNN](../deep_res/cnn_res_deep_ptbdb.ipynb), we also see that we can better exploit these residual connections by making the model much deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Network](cnn_paper.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras import optimizers, losses, activations, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, GlobalAveragePooling1D, \\\n",
    "    concatenate, Add, Activation\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, precision_recall_curve, auc, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PTBDB baseline\n",
    "#### Code mostly from https://github.com/CVxTz/ECG_Heartbeat_Classification/blob/master/code/baseline_ptbdb.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"../data/ptbdb_normal.csv\", header=None)\n",
    "df_2 = pd.read_csv(\"../data/ptbdb_abnormal.csv\", header=None)\n",
    "df = pd.concat([df_1, df_2])\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=1337, stratify=df[187])\n",
    "\n",
    "Y = np.array(df_train[187].values).astype(np.int8)\n",
    "X = np.array(df_train[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "Y_test = np.array(df_test[187].values).astype(np.int8)\n",
    "X_test = np.array(df_test[list(range(187))].values)[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"cnn_paper_ptbdb.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plot_model(model, to_file=\"cnn_paper.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or ALTERNATIVELY train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool_block(X, filters, kernel_size=5, dropout=0.1, pool_size=2):\n",
    "    img_1 = Convolution1D(filters, kernel_size=kernel_size, activation='relu', padding='same')(X)\n",
    "    img_1 = Convolution1D(filters, kernel_size=kernel_size, activation='relu', padding='same')(img_1)\n",
    "    img_1 = Add()([X, img_1])\n",
    "    img_1 = Activation('relu')(img_1)\n",
    "    img_1 = MaxPool1D(pool_size=2)(img_1)\n",
    "    return img_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    nclass = 5\n",
    "    inp = Input(shape=(187, 1))\n",
    "    img_1 = Convolution1D(32, kernel_size=5, activation=activations.relu, padding=\"valid\")(inp)     \n",
    "    for i in range(5):\n",
    "        img_1 = maxpool_block(img_1, 32, dropout=0.2) \n",
    "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"same\")(img_1)  \n",
    "    img_1 = Convolution1D(256, kernel_size=3, activation=activations.relu, padding=\"same\", name=\"final_conv\")(img_1)\n",
    "    img_1 = GlobalMaxPool1D()(img_1)\n",
    "    img_1 = Dropout(rate=0.2)(img_1)\n",
    "\n",
    "    dense_1 = Dense(32, activation=activations.relu, name=\"dense_1\")(img_1)\n",
    "    dense_1 = Dense(32, activation=activations.relu, name=\"dense_2\")(dense_1)\n",
    "    dense_1 = Dense(nclass, activation=activations.softmax, name=\"dense_3_mitbih\")(dense_1)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=dense_1)\n",
    "    opt = optimizers.Adam(0.001)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=losses.sparse_categorical_crossentropy, metrics=['acc'])      \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "file_path = \"cnn_paper_ptbdb.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5, verbose=1)\n",
    "redonplat = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", patience=3, verbose=2)\n",
    "callbacks_list = [checkpoint, early, redonplat]  # early\n",
    "\n",
    "model.fit(X, Y, epochs=1000, verbose=2, callbacks=callbacks_list, validation_split=0.1)\n",
    "model.load_weights(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 score : 0.9914146508349406 \n",
      "Test accuracy score : 0.9931295087598764 \n",
      "AUROC score : 0.989920130219145 \n",
      "AUPRC score : 0.9962858259621176 \n",
      "[[ 795   14]\n",
      " [   6 2096]]\n"
     ]
    }
   ],
   "source": [
    "pred_test = model.predict(X_test)\n",
    "pred_test = (pred_test>0.5).astype(np.int8)\n",
    "\n",
    "f1 = f1_score(Y_test, pred_test, average=\"macro\")\n",
    "\n",
    "print(\"Test f1 score : %s \"% f1)\n",
    "\n",
    "acc = accuracy_score(Y_test, pred_test)\n",
    "\n",
    "print(\"Test accuracy score : %s \"% acc)\n",
    "\n",
    "auc_roc = roc_auc_score(Y_test, pred_test)\n",
    "\n",
    "print(\"AUROC score : %s \"% auc_roc)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(Y_test, pred_test)\n",
    "\n",
    "auc_prc = auc(recall, precision)\n",
    "print(\"AUPRC score : %s \"% auc_prc)\n",
    "\n",
    "print(confusion_matrix(Y_test, pred_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
